{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3YpbpCGqGxdp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "#generator model\n",
        "\n",
        "def build_generator():\n",
        "  model = models.Sequential([\n",
        "      layers.Dense(7*7*128, input_shape=(100,)),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Reshape((7, 7, 128)),\n",
        "      layers.Conv2DTranspose(128,(4,4), strides=(2,2), padding='same'),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Conv2DTranspose(128,(4,4), strides=(2,2), padding='same'),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Conv2D(1,(7,7), padding='same', activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "IFRqgufFH2Iz"
      },
      "outputs": [],
      "source": [
        "#discriminator model\n",
        "\n",
        "def build_discriminator():\n",
        "  model = models.Sequential([\n",
        "      layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28, 28, 1]),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Conv2D(128, (5,5),strides=(2,2),padding='same'),\n",
        "      layers.LeakyReLU(),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jKcI43fnIEJW"
      },
      "outputs": [],
      "source": [
        "#GAN model\n",
        "\n",
        "def build_gan(generator, discriminator):\n",
        "  discriminator.traniable = False\n",
        "  model = models.Sequential([generator, discriminator])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "aRxks0-oJsjI"
      },
      "outputs": [],
      "source": [
        "#load MNIST data\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images,_),(_,_) = mnist.load_data()\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images-127.5) / 127.5 #normalize the image to [-1, -1]\n",
        "#batch size\n",
        "batch_size = 256\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_images.shape[0]).batch(batch_size)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "RqOOkSJMTHyP"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Intialize the model\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "gan = build_gan(generator, discriminator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yBqxHaskLg73"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Intialize the model\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "#Define loss function\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits =True)\n",
        "\n",
        "#Define optimizers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "#Define training setUp\n",
        "@tf.function\n",
        "def train_step(images) :\n",
        "  noise = tf.random.normal([batch_size, 100])\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_images = generator(noise, training=True)\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "    gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "    disc_loss_real = cross_entropy(tf.ones_like(real_output),real_output)\n",
        "    disc_loss_fake = cross_entropy(tf.zeros_like(fake_output),fake_output)\n",
        "    disc_loss = disc_loss_real + disc_loss_fake\n",
        "\n",
        "  gradient_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  gradient_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "  generator_optimizer.apply_gradients(zip(gradient_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradient_of_discriminator, discriminator.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpxeWmVPS6ju"
      },
      "outputs": [],
      "source": [
        "  #traing loop\n",
        "  epochs = 20\n",
        "  for epoch in range(epochs):\n",
        "    for image_batch in train_dataset:\n",
        "      train_step(image_batch)\n",
        "    print(f'Epoch{epoch+1}/{epochs}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJhItz2CUEF4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
